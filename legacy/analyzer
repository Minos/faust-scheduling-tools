#!/usr/bin/env python

from typing import Optional
import math

import argparse
import sys
import csv
import numpy as np
import scipy as sp
import matplotlib
import matplotlib.pyplot as plt


strategies = [
        'deep-first',
        'breadth-first',
        'interleaved',
        'reverse breadth-first',
]


print_warnings = False


class Measure:
    cost: int
    time: float
    gcc_time: float
    size: int
    custom_event: int

    def __init__(self, cost: int, time: float):
        self.cost = cost
        self.time = time
        self.gcc_time = 0
        self.size = 0
        self.custom_event = 0

    def is_valid(self):
        return self.time > 0.0

    def __str__(self):
        s = f"time: {self.time}"
        if self.gcc_time:
            s += f',\tgcc time: {self.gcc_time}'
        if self.size:
            s += f',\tsize: {self.size}'
        if self.custom_event:
            s += f',\tevent: {self.custom_event}'
        return s


class Benchmark:
    dspname: str
    measures: dict[int, Measure]
    mean_time: Optional[float]
    mean_gcc_time: Optional[float]
    mean_cost: Optional[float]
    mean_custom_event: Optional[float]
    has_gcc_times: bool
    has_custom_events: bool

    def __init__(self, dspname: str):
        self.dspname = dspname
        self.measures = {}
        self.mean_time = None
        self.mean_gcc_time = None
        self.mean_cost = None
        self.mean_custom_event = None
        self.has_gcc_times = False
        self.has_custom_events = False

    def add_measure(self, strategy: int, cost: int, time: float):
        self.measures[strategy] = Measure(cost, time)

    def print_strategy_not_found_warning(self, strategy):
        if print_warnings:
            print(f'Warning: strategy {strategy} not found in {self.dspname}.',
                  file=sys.stderr)

    def add_size(self, strategy: int, size: int):
        try:
            self.measures[strategy].size = size
        except KeyError:
            self.print_strategy_not_found_warning(strategy)

    def add_gcc_time(self, strategy: int, gcc_time: float):
        try:
            self.measures[strategy].gcc_time = gcc_time
        except KeyError:
            self.print_strategy_not_found_warning(strategy)

    def add_custom_event(self, strategy: int, custom_event: int):
        try:
            self.measures[strategy].custom_event = custom_event
        except KeyError:
            self.print_strategy_not_found_warning(strategy)

    def get_mean_time(self):
        if not self.mean_time:
            self.mean_time = np.mean(self.time_array())
        return self.mean_time

    def get_mean_gcc_time(self):
        if not self.mean_gcc_time:
            self.mean_gcc_time = np.mean(self.gcc_time_array())
        return self.mean_gcc_time

    def get_mean_cost(self):
        if not self.mean_cost:
            self.mean_cost = np.mean(self.cost_array())
        return self.mean_cost

    def get_mean_custom_event(self):
        if not self.mean_custom_event:
            self.mean_custom_event = np.mean(self.custom_event_array())
        return self.mean_custom_event

    def has_errors(self):
        return any(not m.is_valid() for m in self.measures.values())

    def measure_array(self, function):
        size = len(self.measures)
        array = np.zeros(size)
        for strategy in range(size):
            try:
                array[strategy] = function(self.measures[strategy])
            except KeyError:
                print(f'key not found: {strategy} in {self.measures.keys()}',
                      file=sys.stderr)
        return array

    def cost_array(self):
        return self.measure_array(lambda m: m.cost)

    def time_array(self):
        return self.measure_array(lambda m: m.time)

    def size_array(self):
        return self.measure_array(lambda m: m.size)

    def gcc_time_array(self):
        return self.measure_array(lambda m: m.gcc_time)

    def custom_event_array(self):
        return self.measure_array(lambda m: m.custom_event)

    def max_time_ratio(self) -> float:
        if self.has_gcc_times:
            return np.maximum(
                    max_ratio(self.time_array()),
                    max_ratio(self.gcc_time_array()))
        else:
            return max_ratio(self.time_array())

    def strategy_score(self, strategy: int) -> float:
        return self.measures[strategy].time / self.get_mean_time()

    def __str__(self):
        output = f'{self.dspname}:\t' \
                 f'max time ratio: {self.max_time_ratio():.02f}\n'
        for strategy, measure in self.measures.items():
            output += f'\tstrategy {strategy}:\t{measure}\n'
        return output


class BenchmarkListBuilder:
    def __init__(self):
        self.benchmarks = {}

    def add_measure(self, dspname: str, strategy: int, cost: int, time: float):
        self.get_or_add_benchmark(dspname).add_measure(strategy, cost, time)

    def add_size(self, dspname: str, strategy: int, size: int):
        try:
            self.benchmarks[dspname].add_size(strategy, size)
        except KeyError:
            self.print_dsp_not_found_warning(dspname)

    def add_gcc_time(self, dspname: str, strategy: int, gcc_time: float):
        try:
            self.benchmarks[dspname].add_gcc_time(strategy, gcc_time)
        except KeyError:
            self.print_dsp_not_found_warning(dspname)

    def add_custom_event(self, dspname: str, strategy: int, custom_event: int):
        try:
            self.benchmarks[dspname].add_custom_event(strategy, custom_event)
        except KeyError:
            self.print_dsp_not_found_warning(dspname)

    def print_dsp_not_found_warning(self, dspname):
        if print_warnings:
            print(f'Warning: dsp {dspname} not found.', file=sys.stderr)

    def finalize(self):
        benchmarks = self.benchmarks.values()
        return sorted(benchmarks, key=Benchmark.max_time_ratio, reverse=True)

    def get_or_add_benchmark(self, dspname: str) -> Benchmark:
        try:
            return self.benchmarks[dspname]
        except KeyError:
            benchmark = Benchmark(dspname)
            self.benchmarks[dspname] = benchmark
            return benchmark


def safe_int(value):
    try:
        return int(value)
    except ValueError:
        return 0


def safe_float(value):
    try:
        return float(value)
    except ValueError:
        return 0.0


def parse_output(filename: str,
                 sizes: Optional[str],
                 gcc_times: Optional[str],
                 custom_events: Optional[str]) -> [Benchmark]:
    builder = BenchmarkListBuilder()

    with open(filename, 'r') as file:
        reader = csv.reader(file, delimiter=';')
        for row in reader:
            dspname = row[0]
            strategy = int(row[1])
            cost = safe_int(row[2])
            time = safe_float(row[3])
            builder.add_measure(dspname, strategy, cost, time)

    if sizes:
        with open(sizes, 'r') as file:
            reader = csv.reader(file, delimiter=';')
            for row in reader:
                dspname = row[0]
                strategy = int(row[1])
                size = safe_int(row[2])
                builder.add_size(dspname, strategy, size)

    if gcc_times:
        with open(gcc_times, 'r') as file:
            reader = csv.reader(file, delimiter=';')
            for row in reader:
                dspname = row[0]
                strategy = int(row[1])
                gcc_time = safe_float(row[2])
                builder.add_gcc_time(dspname, strategy, gcc_time)

    if custom_events:
        with open(custom_events, 'r') as file:
            reader = csv.reader(file, delimiter=';')
            for row in reader:
                dspname = row[0]
                strategy = int(row[1])
                custom_event = safe_int(row[2])
                builder.add_custom_event(dspname, strategy, custom_event)

    return builder.finalize()


def max_ratio(array: np.array) -> float:
    array = array[array != 0]
    return np.max(array) / np.min(array)


def normalize(array: np.array) -> np.array:
    mean = np.mean(array)
    return array / mean if mean > 0 else array


def print_benchmarks(benchmarks: [Benchmark]):
    for b in benchmarks:
        print(b)


def print_errors(benchmarks):
    for b in benchmarks.get_errors():
        print(b)


def print_ratio_statistics(benchmarks: [Benchmark],
                           significant_benchmarks: [Benchmark],
                           ratio_min: float):
    total = len(benchmarks)
    number_considered = len(significant_benchmarks)
    number_ignored = total - number_considered
    print(f'{number_considered} measures considered '
          f'({100 * (number_considered / total):.02f}%); '
          f'{number_ignored} measures falled below ratio {ratio_min} '
          f'({100 * (number_ignored / total):.02f}%).')


def print_strategy_scores(benchmarks: [Benchmark], n_strategies: int = 4):
    for strategy in range(n_strategies):
        scores = np.asarray([b.strategy_score(strategy) for b in benchmarks])
        print(f'strategy {strategy}: '
              f'mean {np.mean(scores):.04f}, '
              f'variance {np.var(scores):.04f}')


def print_cost_time_correlation(benchmarks: [Benchmark]):
    costs = np.concat([normalize(b.cost_array())
                       for b in benchmarks if not b.has_errors()])
    times = np.concat([normalize(b.time_array())
                       for b in benchmarks if not b.has_errors()])
    correlation = float(sp.stats.pearsonr(costs, times).statistic)
    print(f'cost estimation correlation with clang time: {correlation:.04f}')


def print_size_time_correlation(benchmarks: [Benchmark]):
    sizes = np.concat([normalize(b.size_array())
                       for b in benchmarks if not b.has_errors()])
    times = np.concat([normalize(b.time_array())
                       for b in benchmarks if not b.has_errors()])
    correlation = float(sp.stats.pearsonr(sizes, times).statistic)
    print(f'assembly size correlation with clang time: {correlation:.04f}')


def print_gcc_time_correlation(benchmarks: [Benchmark]):
    times = np.concat([normalize(b.time_array())
                       for b in benchmarks if not b.has_errors()])
    gcc_times = np.concat([normalize(b.gcc_time_array())
                           for b in benchmarks if not b.has_errors()])
    correlation = float(sp.stats.pearsonr(gcc_times, times).statistic)
    print(f'gcc time correlation with clang time: {correlation:.04f}')


def print_custom_event_correlation(benchmarks: [Benchmark]):
    times = np.concat([normalize(b.time_array())
                       for b in benchmarks if not b.has_errors()])
    custom_events = np.concat([normalize(b.custom_event_array())
                               for b in benchmarks if not b.has_errors()])
    correlation = float(sp.stats.pearsonr(times, custom_events).statistic)
    print(f'event correlation with clang time: {correlation:.04f}')


def plot_benchmarks(benchmarks: [Benchmark],
                    title,
                    bar_function,
                    aux_function=None,
                    *,
                    label_function=None,
                    output_file=None):
    y = np.arange(len(benchmarks))
    height = 1 / (len(strategies) + 1)
    thickness = 0.8

    fig, ax = plt.subplots(figsize=(6, 7))

    for i, strategy in enumerate(strategies):
        offset = height * i

        measures = [bar_function(b, i) for b in benchmarks]
        rects = ax.barh(y + offset, measures, height * thickness,
                        alpha=0.75 if aux_function else 1,
                        label=strategy)

        if aux_function:
            measures  = [aux_function(b, i) for b in benchmarks]
            # rects = ax.scatter(measures, y + offset, c='black', marker='o')
            color = rects.patches[0].get_facecolor()
            ax.stem(y + offset, measures, linefmt=f'k-', markerfmt='.',
                    orientation='horizontal')
            # color = rects.patches[0].get_facecolor()
            # rects = ax.barh(y + offset, measures, height * thickness * 0.5,
            #                 color=color, alpha=1, label=strategy)

        if label_function and not aux_function:
            labels = [label_function(b, i) for b in benchmarks]
            ax.bar_label(rects, labels=labels, padding=3, fontsize=6)

    ax.set_title(title)
    ax.set_ylabel('Programme')
    ax.set_xlim(0, 2.75)
    yticks = [b.dspname for b in benchmarks]
    ax.set_yticks(y + height * (len(strategies) // 2 - 0.5), yticks)
    ax.invert_yaxis()
    ax.legend(loc='center right', ncols=1)

    if output_file:
        plt.savefig(output_file, bbox_inches='tight')
    else:
        plt.show()


def plot_times(benchmarks: [Benchmark], output_file=None):
    plot_benchmarks(
            benchmarks,
            'Temps de calcul par stratégie, par rapport au temps moyen',
            lambda b, i: b.measures[i].time / b.get_mean_time(),
            label_function=lambda b, i: f'{b.measures[i].time:.02f}ms',
            output_file=output_file)


def plot_gcc_times(benchmarks: [Benchmark], output_file=None):
    plot_benchmarks(
            benchmarks,
            'Temps de calcul par stratégie, par rapport au temps moyen',
            lambda b, i: b.measures[i].gcc_time / b.get_mean_gcc_time(),
            label_function=lambda b, i: f'{b.measures[i].gcc_time:.02f}ms',
            output_file=output_file)


def plot_estimation(benchmarks: [Benchmark], output_file=None):
    title = 'Estimation du coût par stratégie, '\
            'par rapport à l\'estimation moyenne'
    plot_benchmarks(
            benchmarks,
            title,
            lambda b, i: b.measures[i].time / b.get_mean_time(),
            lambda b, i: b.measures[i].cost / b.get_mean_cost(),
            label_function=lambda b, i: f'{b.measures[i].cost}',
            output_file=output_file)


def plot_custom_events(benchmarks: [Benchmark],
                       event_name: str,
                       output_file=None):
    plot_benchmarks(
            benchmarks,
            f'Nombre de {event_name} par rapport au nombre moyen',
            lambda b, i: b.measures[i].time / b.get_mean_time(),
            lambda b, i: b.measures[i].custom_event / b.get_mean_custom_event(),
            label_function=lambda b, i: f'{b.measures[i].custom_event}',
            output_file=output_file)


def plot_max_ratio(benchmarks: [Benchmark], output_file=None):
    fig, ax = plt.subplots(figsize=(7, 2))

    x = np.arange(len(benchmarks))
    y = np.asarray([b.max_time_ratio() for b in benchmarks])
    ax.semilogy()
    ax.bar(x, y, width=1, color='tab:purple')
    ax.yaxis.set_major_formatter(matplotlib.ticker.ScalarFormatter())
    ax.set_yticks(list(range(1, math.ceil(np.max(y) + 1))))

    ax.set_title('Distribution des rapports de temps d’exécution')

    if output_file:
        plt.savefig(output_file, bbox_inches='tight')
    else:
        plt.show()


def main():
    parser = argparse.ArgumentParser(
            prog='Analyzer',
            description='Analyze fcbenchtool output')

    parser.add_argument('filename', help='output file to analyze')
    parser.add_argument('-s', '--sizes', default=None,
                        help='asm size file to analyze')
    parser.add_argument('-g', '--gcc', default=None,
                        help='gcc times file to analyze')
    parser.add_argument('-e', '--events', default=None,
                        help='perf event csv')
    parser.add_argument('-r', '--ratio-min', default=1,
                        help='only considers benchmarks with at least this '
                             'ratio between minimum and maximum time')
    parser.add_argument('-p', '--print', action='store_true',
                        help='print detailed measures')
    parser.add_argument('-w', '--print-warnings', action='store_true',
                        help='print warnings')
    parser.add_argument('--plot-times', action='store_true',
                        help='plot times')
    parser.add_argument('--plot-gcc-times', action='store_true',
                        help='plot gcc times')
    parser.add_argument('--plot-estimations', action='store_true',
                        help='plot cost estimation')
    parser.add_argument('--plot-events', default=None,
                        help='plot given perf events')
    parser.add_argument('--plot-max-ratio', action='store_true',
                        help='plot maximum ratio histogram')
    parser.add_argument('-o', '--output', default=None,
                        help='output plot to file')

    args = parser.parse_args()

    global print_warnings
    print_warnings = args.print_warnings

    benchmarks = parse_output(args.filename, args.sizes, args.gcc, args.events)
    ratio_min = float(args.ratio_min)

    plt.style.use('../report.mplstyle')

    # When outputing to png format, we need a higher DPI.
    if args.output:
        plt.rcParams['figure.dpi'] = 512

    significant_benchmarks = [b for b in benchmarks
                              if b.max_time_ratio() >= ratio_min]

    if args.print:
        print_benchmarks(significant_benchmarks)

    print_ratio_statistics(benchmarks, significant_benchmarks, ratio_min)
    print_strategy_scores(significant_benchmarks)
    print_cost_time_correlation(significant_benchmarks)

    if args.sizes:
        print_size_time_correlation(significant_benchmarks)

    if args.gcc:
        print_gcc_time_correlation(significant_benchmarks)

    if args.events:
        print_custom_event_correlation(significant_benchmarks)

    if args.plot_times:
        plot_times(significant_benchmarks, args.output)
    elif args.plot_gcc_times:
        plot_gcc_times(significant_benchmarks, args.output)
    elif args.plot_estimations:
        plot_estimation(significant_benchmarks, args.output)
    elif args.plot_events:
        plot_custom_events(significant_benchmarks, args.plot_events, args.output)
    elif args.plot_max_ratio:
        plot_max_ratio(significant_benchmarks, args.output)


if __name__ == "__main__":
    main()
